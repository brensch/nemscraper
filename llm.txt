# Directory Context: src/process2/
# Generated by dir-to-context tool
# Total files processed below


## trimming.rs
```
use crate::process::utils::clean_str;
use anyhow::Result;
use arrow::{
    array::{ArrayRef, StringArray},
    record_batch::RecordBatch,
};
use std::sync::Arc;

/// Apply trimming to flagged columns
pub fn apply_trimming(batch: &RecordBatch, trim_columns: &[String]) -> Result<RecordBatch> {
    if trim_columns.is_empty() {
        return Ok(batch.clone());
    }

    let mut cols = Vec::with_capacity(batch.num_columns());
    for (i, field) in batch.schema().fields().iter().enumerate() {
        let arr = batch.column(i);
        if trim_columns.contains(field.name()) {
            if let Some(sarr) = arr.as_any().downcast_ref::<StringArray>() {
                let trimmed: StringArray = sarr.iter().map(|opt| opt.map(clean_str)).collect();
                cols.push(Arc::new(trimmed) as ArrayRef);
                continue;
            }
        }
        cols.push(arr.clone());
    }

    RecordBatch::try_new(batch.schema(), cols).map_err(Into::into)
}
```


## split.rs
```
use anyhow::{Context, Result};
use futures_util::StreamExt;
use reqwest;
use std::io::{BufRead, BufReader, Cursor};
use std::path::Path;
use tracing::{debug, instrument, warn};
use zip::ZipArchive;

// Import the unified processor
use super::csv_processor::process_csv_entry_unified;

/// A simple accumulator for rows and bytes.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct RowsAndBytes {
    pub rows: u64,
    pub bytes: u64,
}

impl RowsAndBytes {
    pub const ZERO: Self = RowsAndBytes { rows: 0, bytes: 0 };

    /// Add another RowsAndBytes into `self`, saturating on overflow.
    pub fn add(&mut self, other: RowsAndBytes) {
        self.rows = self.rows.saturating_add(other.rows);
        self.bytes = self.bytes.saturating_add(other.bytes);
    }
}

/// Streams a ZIP file from URL, processes each CSV into chunks by D-row count.
/// Uses minimal memory by streaming download and processing entries sequentially.
#[instrument(level = "debug", skip(url, out_dir), fields(url = %url))]
pub async fn stream_zip_to_parquet<P: AsRef<Path>>(url: &str, out_dir: P) -> Result<RowsAndBytes> {
    let max_data_rows_per_chunk = 64_000;
    debug!("Starting download from: {}", url);

    // Stream the ZIP file from URL
    let client = reqwest::Client::new();
    let response = client
        .get(url)
        .send()
        .await
        .with_context(|| format!("Failed to send request to {}", url))?;

    if !response.status().is_success() {
        return Err(anyhow::anyhow!("HTTP error: {}", response.status()));
    }

    // Stream the response body into memory with progress tracking
    let mut zip_data = Vec::new();
    let mut stream = response.bytes_stream();
    let mut total_downloaded = 0u64;

    while let Some(chunk) = stream.next().await {
        let chunk = chunk.context("Failed to read chunk from response")?;
        zip_data.extend_from_slice(&chunk);
        total_downloaded += chunk.len() as u64;

        // Log progress every 10MB
        if total_downloaded % (10 * 1024 * 1024) == 0 {
            debug!("Downloaded {} MB", total_downloaded / (1024 * 1024));
        }
    }

    debug!("Download complete: {} bytes", zip_data.len());

    // Process the ZIP from memory
    let cursor = Cursor::new(zip_data);
    let mut archive =
        ZipArchive::new(cursor).context("Failed to create ZipArchive from downloaded data")?;

    let mut totals = RowsAndBytes::ZERO;

    for idx in 0..archive.len() {
        let entry = archive.by_index(idx)?;
        let name = entry.name().to_string();

        if !name.to_lowercase().ends_with(".csv") {
            continue;
        }

        debug!("Processing CSV entry: {}", name);
        let reader = BufReader::new(entry);

        let (rows, bytes) =
            process_csv_entry_unified(reader, &name, out_dir.as_ref(), max_data_rows_per_chunk)
                .with_context(|| format!("processing CSV entry {}", name))?;

        let entry_counts = RowsAndBytes { rows, bytes };
        totals.add(entry_counts);

        debug!(
            "Entry {} complete: {} rows, {} bytes",
            name, entry_counts.rows, entry_counts.bytes
        );
    }

    debug!(
        "Completed ZIP: total rows = {}, total bytes = {}",
        totals.rows, totals.bytes
    );
    Ok(totals)
}

#[cfg(test)]
mod tests {
    use super::*;
    use anyhow::Result;
    use glob::glob;
    use tempfile::tempdir;
    use tokio;
    use tracing_subscriber::{fmt, EnvFilter};

    fn init_logging() {
        let _ = fmt()
            .with_env_filter(EnvFilter::new("debug"))
            .with_target(false)
            .try_init();
    }

    #[tokio::test]
    async fn integration_stream_zip_to_parquet() -> Result<()> {
        init_logging();

        // Test with a public ZIP file URL
        let url = "https://example.com/test.zip"; // Replace with actual test URL
        let output_dir = tempdir()?;
        let max_rows_per_chunk = 10_000; // 10k D-rows per chunk

        let RowsAndBytes { rows, bytes } = stream_zip_to_parquet(url, output_dir.path()).await?;

        assert!(rows > 0, "Expected at least one row, got {}", rows);
        assert!(bytes > 0, "Expected some bytes, got {}", bytes);

        // Check for generated parquet files
        let pattern = format!("{}/**/*.parquet", output_dir.path().display());
        let parquet_files: Vec<_> = glob(&pattern)?.filter_map(Result::ok).collect();
        assert!(!parquet_files.is_empty(), "No .parquet files found");

        Ok(())
    }
}
```


## date_parser.rs
```
use chrono::{FixedOffset, NaiveDate, TimeZone};

/// Fast parse of `"YYYY/MM/DD HH:MM:SS"` → millis UTC
pub fn parse_timestamp_millis(s: &str) -> Option<i64> {
    let s = s.trim();
    // minimal length + separators check
    if s.len() < 19 || &s[4..5] != "/" || &s[7..8] != "/" || &s[10..11] != " " {
        return None;
    }
    let year: i32 = s[0..4].parse().ok()?;
    let month: u32 = s[5..7].parse().ok()?;
    let day: u32 = s[8..10].parse().ok()?;
    let hour: u32 = s[11..13].parse().ok()?;
    let min: u32 = s[14..16].parse().ok()?;
    let sec: u32 = s[17..19].parse().ok()?;

    let naive = NaiveDate::from_ymd_opt(year, month, day)?.and_hms_opt(hour, min, sec)?;
    let offset = FixedOffset::east_opt(10 * 3600).unwrap();
    offset
        .from_local_datetime(&naive)
        .single()
        .map(|dt| dt.timestamp_millis())
}
```


## csv_processor.rs
```
use anyhow::{anyhow, Context, Result};
use arrow::{
    csv::ReaderBuilder,
    datatypes::{DataType, Field, Schema},
    record_batch::RecordBatch,
};
use parquet::{
    arrow::ArrowWriter,
    basic::{BrotliLevel, Compression},
    file::properties::WriterProperties,
};
use std::{
    fs::{self, File},
    io::Cursor,
    path::Path,
    sync::Arc,
};
use tracing::{debug, warn};

use crate::process::{
    convert::convert_to_final_types,
    schema::{analyze_batch_for_schema, SchemaInfo},
    trimming::apply_trimming,
    utils::{clean_str, extract_date_from_filename},
};

/// Parse CSV fields respecting quotes and escapes
fn parse_csv_fields(line: &str) -> Vec<String> {
    let mut fields = Vec::new();
    let mut current_field = String::new();
    let mut in_quotes = false;
    let mut chars = line.chars().peekable();

    while let Some(ch) = chars.next() {
        match ch {
            '"' if !in_quotes => {
                in_quotes = true;
            }
            '"' if in_quotes => {
                // Check for escaped quote
                if chars.peek() == Some(&'"') {
                    current_field.push('"');
                    chars.next(); // consume the second quote
                } else {
                    in_quotes = false;
                }
            }
            ',' if !in_quotes => {
                fields.push(current_field.trim().to_string());
                current_field.clear();
            }
            _ => {
                current_field.push(ch);
            }
        }
    }
    fields.push(current_field.trim().to_string());
    fields
}

/// Unified processor that chunks by D-row count and handles I-line schema changes
pub struct UnifiedCsvProcessor {
    /// Maximum number of D-rows per chunk
    max_data_rows: usize,

    /// Current schema line (I-line)
    current_schema_line: Option<String>,

    /// Accumulated data lines for current chunk
    data_lines: Vec<String>,

    /// Schema information derived from current I-line (after dropping first 4 cols)
    schema_info: Option<SchemaInfo>,

    /// Output directory for this file
    output_dir: std::path::PathBuf,

    /// Base filename for output
    file_name: String,

    /// Chunk counter for unique filenames
    chunk_index: usize,

    /// Total rows processed
    total_data_rows: u64,

    /// Total parquet bytes written
    total_parquet_bytes: u64,
}

impl UnifiedCsvProcessor {
    pub fn new(max_data_rows: usize, file_name: String, output_dir: &Path) -> Result<Self> {
        fs::create_dir_all(output_dir).context("creating output directory")?;

        Ok(Self {
            max_data_rows,
            current_schema_line: None,
            data_lines: Vec::with_capacity(max_data_rows),
            schema_info: None,
            output_dir: output_dir.to_path_buf(),
            file_name,
            chunk_index: 0,
            total_data_rows: 0,
            total_parquet_bytes: 0,
        })
    }

    /// Process a single line from the CSV
    pub fn process_line(&mut self, line: &str) -> Result<()> {
        let trimmed = line.trim_start();

        if trimmed.starts_with("I,") {
            // New schema line - flush current chunk if we have data
            if !self.data_lines.is_empty() {
                self.flush_current_chunk()?;
            }

            // Parse and validate the I-line
            let fields = parse_csv_fields(line);
            debug!(
                "Found I-line with {} fields: {}",
                fields.len(),
                line.chars().take(200).collect::<String>()
            );

            // Start new schema context
            self.current_schema_line = Some(line.to_string());
            self.schema_info = None; // Will be derived on first data row
        } else if trimmed.starts_with("D,") {
            // Data line - add to current chunk
            if self.current_schema_line.is_none() {
                warn!("Found data line before schema line, skipping");
                return Ok(());
            }

            self.data_lines.push(line.to_string());

            // Debug first few data lines
            if self.data_lines.len() <= 3 {
                let fields = parse_csv_fields(line);
                debug!(
                    "Data line {} has {} fields: {}",
                    self.data_lines.len(),
                    fields.len(),
                    line.chars().take(200).collect::<String>()
                );
            }

            // Flush if chunk is full
            if self.data_lines.len() >= self.max_data_rows {
                self.flush_current_chunk()?;
            }
        }
        // Ignore other lines (C, etc.)

        Ok(())
    }

    /// Flush current accumulated data lines to a parquet file
    fn flush_current_chunk(&mut self) -> Result<()> {
        if self.data_lines.is_empty() {
            return Ok(());
        }

        let schema_line = self
            .current_schema_line
            .as_ref()
            .ok_or_else(|| anyhow!("No schema line available"))?;

        // Build complete CSV content with schema + data (dropping first 4 columns)
        let csv_content = self.build_csv_content_drop_first_4(schema_line)?;

        // Convert to parquet in one go
        let parquet_bytes = self.csv_to_parquet(&csv_content)?;

        self.total_data_rows += self.data_lines.len() as u64;
        self.total_parquet_bytes += parquet_bytes;
        self.chunk_index += 1;

        // Clear for next chunk (but keep schema_line for reuse)
        self.data_lines.clear();

        debug!(
            "Flushed chunk {} with {} rows, {} bytes",
            self.chunk_index - 1,
            self.data_lines.len(),
            parquet_bytes
        );

        Ok(())
    }

    /// Build CSV content efficiently, dropping first 4 columns to save space
    fn build_csv_content_drop_first_4(&self, schema_line: &str) -> Result<String> {
        // Parse schema fields and validate
        let schema_fields = parse_csv_fields(schema_line);
        if schema_fields.len() < 5 {
            return Err(anyhow!(
                "Schema line has fewer than 5 fields, cannot drop first 4"
            ));
        }

        // Drop first 4 fields from schema (I, and 3 data columns)
        let trimmed_schema_fields = &schema_fields[4..];

        // Pre-calculate size to avoid reallocations
        let estimated_line_len =
            schema_line.len() / schema_fields.len() * trimmed_schema_fields.len();
        let total_data_lines = self.data_lines.len();
        let total_size = estimated_line_len * (total_data_lines + 1); // +1 for header

        let mut csv = String::with_capacity(total_size);

        // Build new header line
        csv.push_str(&trimmed_schema_fields.join(","));
        csv.push('\n');

        // Validate and include data lines (dropping first 4 columns)
        let mut valid_lines = 0;
        for (i, line) in self.data_lines.iter().enumerate() {
            let fields = parse_csv_fields(line);

            if fields.len() != schema_fields.len() {
                warn!(
                    "Data line {} has {} fields, expected {}: {}",
                    i,
                    fields.len(),
                    schema_fields.len(),
                    line.chars().take(100).collect::<String>()
                );
                // Skip malformed lines rather than failing the entire chunk
                continue;
            }

            // Drop first 4 fields (D, and 3 data columns) and join the rest
            if fields.len() >= 4 {
                let trimmed_data_fields = &fields[4..];
                csv.push_str(&trimmed_data_fields.join(","));
                csv.push('\n');
                valid_lines += 1;
            }
        }

        if valid_lines == 0 {
            return Err(anyhow!("No valid data lines found in chunk"));
        }

        debug!(
            "Built CSV content with {} valid lines out of {}, dropped first 4 columns",
            valid_lines,
            self.data_lines.len()
        );
        Ok(csv)
    }

    /// Convert CSV content to parquet file
    fn csv_to_parquet(&mut self, csv_content: &str) -> Result<u64> {
        // Parse headers from the trimmed CSV content (first line)
        let first_line = csv_content
            .lines()
            .next()
            .ok_or_else(|| anyhow!("No header line in CSV content"))?;

        let headers: Vec<String> = first_line.split(',').map(clean_str).collect();

        // debug!(
        //     "Processing chunk with {} headers (after dropping first 4): {:?}",
        //     headers.len(),
        //     headers
        // );

        // Derive schema info if not already done
        if self.schema_info.is_none() {
            self.schema_info = Some(
                self.infer_schema(&headers, csv_content)
                    .context("failed to infer schema")?,
            );
        }
        let schema_info = self.schema_info.as_ref().unwrap();

        // Determine output path
        let output_path = self
            .determine_output_path(&schema_info.table_name)
            .context("failed to determine output path")?;

        // Convert CSV to Arrow batch
        let batch = self
            .csv_to_arrow_batch(csv_content, &headers)
            .context("failed to convert CSV to Arrow batch")?;

        // Apply transformations (trimming and date parsing)
        let trimmed = apply_trimming(&batch, &schema_info.trim_columns)
            .context("failed to apply trimming")?;
        let final_batch = convert_to_final_types(&trimmed, schema_info)
            .context("failed to convert to final types")?;

        // Write to parquet
        self.write_parquet_file(&final_batch, &schema_info.schema, &output_path)
            .context("failed to write parquet file")
    }

    fn infer_schema(&self, headers: &[String], csv_content: &str) -> Result<SchemaInfo> {
        // Create temporary string schema for inference
        let fields: Vec<Field> = headers
            .iter()
            .map(|n| Field::new(n, DataType::Utf8, true))
            .collect();
        let string_schema = Schema::new(fields);

        // Parse a small sample for schema inference with robust settings
        let cursor = Cursor::new(csv_content.as_bytes());
        let mut reader = ReaderBuilder::new(Arc::new(string_schema))
            .with_header(true)
            .with_batch_size(1_000)
            .with_quote(b'"')
            .with_escape(b'"')
            .with_delimiter(b',')
            .build(cursor)
            .context("creating CSV reader for schema inference")?;

        let first_batch = reader.next().ok_or_else(|| anyhow!("No data in CSV"))??;

        analyze_batch_for_schema(&first_batch, headers)
    }

    fn determine_output_path(&self, table_name: &str) -> Result<std::path::PathBuf> {
        // Extract partition date from filename
        let partition_date = extract_date_from_filename(&self.file_name).unwrap_or_else(|| {
            warn!(
                "No date found in filename '{}', using default",
                self.file_name
            );
            "unknown-date".to_string()
        });

        // Create table/partition directory structure
        let table_dir = self.output_dir.join(table_name);
        fs::create_dir_all(&table_dir).context("creating table directory")?;

        let partition_dir = table_dir.join(format!("date={}", partition_date));
        fs::create_dir_all(&partition_dir).context("creating partition directory")?;

        // Generate unique filename for this chunk
        let filename = format!("{}-chunk-{}.parquet", self.file_name, self.chunk_index);
        Ok(partition_dir.join(filename))
    }

    fn csv_to_arrow_batch(&self, csv_content: &str, headers: &[String]) -> Result<RecordBatch> {
        // Create string schema for parsing
        let fields: Vec<Field> = headers
            .iter()
            .map(|n| Field::new(n, DataType::Utf8, true))
            .collect();
        let schema = Schema::new(fields);

        let cursor = Cursor::new(csv_content.as_bytes());
        let mut reader = ReaderBuilder::new(Arc::new(schema))
            .with_header(true)
            .with_batch_size(self.max_data_rows)
            .with_quote(b'"') // Handle quoted fields
            .with_escape(b'"') // Handle escaped quotes
            .with_delimiter(b',')
            .build(cursor)
            .context("creating CSV reader")?;

        match reader.next() {
            Some(Ok(batch)) => Ok(batch),
            Some(Err(e)) => {
                // Log the problematic CSV content for debugging
                let lines: Vec<&str> = csv_content.lines().take(5).collect();
                warn!("CSV parsing failed. First few lines: {:?}", lines);
                warn!("Expected {} fields, error: {}", headers.len(), e);
                Err(e).context("reading CSV batch")
            }
            None => Err(anyhow!("No data in CSV batch")),
        }
    }

    fn write_parquet_file(
        &self,
        batch: &RecordBatch,
        schema: &Schema,
        output_path: &Path,
    ) -> Result<u64> {
        let file = File::create(output_path)
            .with_context(|| format!("creating file {}", output_path.display()))?;

        let props = WriterProperties::builder()
            .set_compression(Compression::BROTLI(BrotliLevel::try_new(5)?))
            .build();

        let mut writer = ArrowWriter::try_new(file, Arc::new(schema.clone()), Some(props))
            .context("creating parquet writer")?;

        writer.write(batch).context("writing batch to parquet")?;
        writer.close().context("closing parquet writer")?;

        let metadata = fs::metadata(output_path).context("getting file metadata")?;

        Ok(metadata.len())
    }

    /// Finalize processing - flush any remaining data
    pub fn finalize(mut self) -> Result<(u64, u64)> {
        if !self.data_lines.is_empty() {
            self.flush_current_chunk()?;
        }
        Ok((self.total_data_rows, self.total_parquet_bytes))
    }
}

/// Simplified entry point for processing CSV entries
pub fn process_csv_entry_unified<R: std::io::BufRead>(
    mut reader: R,
    file_name: &str,
    out_dir: &Path,
    max_data_rows: usize,
) -> Result<(u64, u64)> {
    // Skip top C-header if present
    let mut first_line = String::new();
    reader.read_line(&mut first_line)?;

    let mut processor = UnifiedCsvProcessor::new(max_data_rows, file_name.to_string(), out_dir)?;

    // If first line wasn't a C-header, process it
    if !first_line.trim_start().starts_with('C') {
        processor.process_line(&first_line)?;
    }

    // Process remaining lines
    let mut line = String::new();
    loop {
        line.clear();
        let bytes_read = reader.read_line(&mut line)?;
        if bytes_read == 0 {
            break; // EOF
        }

        let trimmed = line.trim_start();
        if trimmed.starts_with("C,") {
            break; // Footer reached
        }

        processor.process_line(&line)?;
    }

    processor.finalize()
}
```


## schema.rs
```
use crate::process::date_parser;
use crate::process::utils::{clean_str, infer_arrow_dtype_from_str};
use anyhow::Result;
use arrow::{
    array::StringArray,
    datatypes::{DataType, Field, Schema, TimeUnit},
    record_batch::RecordBatch,
};

/// Holds final schema + which cols need date‐parsing or trimming
pub struct SchemaInfo {
    pub schema: Schema,
    pub date_columns: Vec<String>,
    pub trim_columns: Vec<String>,
    pub table_name: String,
}

/// Analyze first batch to detect dates, numbers, and trims
pub fn analyze_batch_for_schema(batch: &RecordBatch, headers: &[String]) -> Result<SchemaInfo> {
    let mut final_fields = Vec::with_capacity(headers.len());
    let mut date_columns = Vec::new();
    let mut trim_columns = Vec::new();

    for (i, name) in headers.iter().enumerate() {
        let col = batch.column(i);
        if let Some(sarr) = col.as_any().downcast_ref::<StringArray>() {
            if let Some(raw) = sarr.iter().find_map(|v| v) {
                let cleaned = clean_str(raw);
                if cleaned != raw {
                    trim_columns.push(name.clone());
                }
                if date_parser::parse_timestamp_millis(&cleaned).is_some() {
                    date_columns.push(name.clone());
                    final_fields.push(Field::new(
                        name,
                        DataType::Timestamp(TimeUnit::Millisecond, Some("+10:00".into())),
                        true,
                    ));
                    continue;
                }
                let ty = infer_arrow_dtype_from_str(&cleaned);
                final_fields.push(Field::new(name, ty, true));
                continue;
            }
        }
        final_fields.push(Field::new(name, DataType::Utf8, true));
    }

    let table_name = if headers.len() >= 4 {
        format!("{}---{}---{}", headers[1], headers[2], headers[3])
    } else {
        "default_table".into()
    };

    Ok(SchemaInfo {
        schema: Schema::new(final_fields),
        date_columns,
        trim_columns,
        table_name,
    })
}
```


## convert.rs
```
use crate::process::schema::SchemaInfo;
use crate::process::{date_parser, utils};
use anyhow::Result;
use arrow::{
    array::{ArrayRef, Float64Builder, StringArray, TimestampMillisecondBuilder},
    datatypes::{DataType, TimeUnit},
    record_batch::RecordBatch,
};
use std::sync::Arc;

/// Convert string columns into your final types
pub fn convert_to_final_types(
    batch: &RecordBatch,
    schema_info: &SchemaInfo,
) -> Result<RecordBatch> {
    let mut out = Vec::with_capacity(batch.num_columns());

    for (arr, fld) in batch.columns().iter().zip(schema_info.schema.fields()) {
        match (arr.as_any().downcast_ref::<StringArray>(), fld.data_type()) {
            // Date → timestamp
            (Some(sarr), DataType::Timestamp(TimeUnit::Millisecond, _))
                if schema_info.date_columns.contains(fld.name()) =>
            {
                let mut b = TimestampMillisecondBuilder::new();
                for opt in sarr.iter() {
                    let ts = opt.and_then(|s| {
                        let c = utils::clean_str(s);
                        date_parser::parse_timestamp_millis(&c)
                    });
                    b.append_option(ts);
                }
                let col = b.finish().with_timezone("+10:00");
                out.push(Arc::new(col) as ArrayRef);
            }

            // Numeric → f64
            (Some(sarr), DataType::Float64) => {
                let mut b = Float64Builder::new();
                for opt in sarr.iter() {
                    let v = opt.and_then(|s| utils::clean_str(s).parse().ok());
                    b.append_option(v);
                }
                out.push(Arc::new(b.finish()) as ArrayRef);
            }

            // Everything else
            _ => out.push(arr.clone()),
        }
    }

    let schema = Arc::new(schema_info.schema.clone());
    RecordBatch::try_new(schema, out).map_err(Into::into)
}
```


## mod.rs
```
pub mod convert;
pub mod csv_processor; // New unified processor
pub mod date_parser;
pub mod schema;
pub mod split;
pub mod trimming;
pub mod utils;

// Remove the old modules:
// pub mod chunk;
// pub mod csv_batch_processor;
```


## utils.rs
```
use arrow::datatypes::DataType;

/// 1) Trim whitespace + strip outer quotes if present.
pub fn clean_str(raw: &str) -> String {
    let trimmed = raw.trim();
    if trimmed.starts_with('"') && trimmed.ends_with('"') && trimmed.len() >= 2 {
        trimmed[1..trimmed.len() - 1].to_string()
    } else {
        trimmed.to_string()
    }
}

/// 2) Infer Arrow dtype from a cleaned string
pub fn infer_arrow_dtype_from_str(s: &str) -> DataType {
    if s.parse::<f64>().is_ok() {
        DataType::Float64
    } else {
        DataType::Utf8
    }
}

/// Extracts a date partition string from the filename, trying:
///  - an 8-digit contiguous `YYYYMMDD`
///  - a 10-char `YYYY-MM-DD` or `YYYY_MM_DD`
/// Returns `Some("YYYY-MM-DD")` if found, else `None`.
pub fn extract_date_from_filename(filename: &str) -> Option<String> {
    let chars: Vec<char> = filename.chars().collect();

    // Try YYYYMMDD
    for i in 0..=chars.len().saturating_sub(8) {
        let slice = &chars[i..i + 8];
        if slice.iter().all(|c| c.is_ascii_digit()) {
            let s: String = slice.iter().collect();
            let (y, m, d) = (
                &s[0..4].parse::<u32>().ok()?,
                &s[4..6].parse::<u32>().ok()?,
                &s[6..8].parse::<u32>().ok()?,
            );
            if (2000..=2030).contains(y) && (1..=12).contains(m) && (1..=31).contains(d) {
                return Some(format!("{:04}-{:02}-{:02}", y, m, d));
            }
        }
    }

    // Try YYYY-MM-DD or YYYY_MM_DD
    for i in 0..=chars.len().saturating_sub(10) {
        let slice: String = chars[i..i + 10].iter().collect();
        if &slice[4..5] == "-" || &slice[4..5] == "_" {
            let sep = &slice[4..5];
            let parts: Vec<&str> = slice.split(sep).collect();
            if parts.len() == 3 {
                let (y, m, d) = (
                    parts[0].parse::<u32>().ok()?,
                    parts[1].parse::<u32>().ok()?,
                    parts[2].parse::<u32>().ok()?,
                );
                if (2000..=2030).contains(&y) && (1..=12).contains(&m) && (1..=31).contains(&d) {
                    return Some(format!("{:04}-{:02}-{:02}", y, m, d));
                }
            }
        }
    }

    None
}
```


---
# Summary
- Total files processed: 8
- Total content size: 27953 bytes
- Source directory: src/process2/
